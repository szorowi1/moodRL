{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mood-RL Model Fitting\n",
    "## Section 1: Motivating model parameterization  \n",
    "### Explaining the \"Matt trick\"\n",
    "To fit reinforcement learning models with Stan, we utilize non-centered parameterizations (aka the \"Matt trick\"). This parameterization improves the efficiency of MCMC sampling by making the sampling space uniformly dense. Traditionally in Bayesian modeling, the inverse temperature ($\\beta$) and learning rate ($\\eta$) are sampled from directly (often with non-informative uniform priors). We will instead employ a two-step procedure where we will first sample from independent unit normal priors and then transform the samples into their appropriate parameter spaces:\n",
    "\n",
    "> $\\beta_{pr} \\ \\text{~} \\mathcal{N}(0,1) $ \n",
    "\n",
    "> $\\beta = C \\cdot \\text{Phi_approx}(\\beta_{pr})$\n",
    "\n",
    "> $\\eta_{pr} \\ \\text{~} \\mathcal{N}(0,1) $ \n",
    "\n",
    "> $\\eta = \\text{Phi_approx}(\\eta_{pr}) $\n",
    "\n",
    "where *Phi_approx* is a fast approximation of the cumulative unit normal [(Bowling et al., 2009)](http://www.jiem.org/index.php/jiem/article/view/60):\n",
    "\n",
    "> $\\text{Phi_approx}(x) = \\text{logit}^{-1}(0.07056 \\ x^3 + 1.5976 \\ x) $\n",
    "\n",
    "As an approximation of the cumulative unit normal, the *Phi_approx* function conveniently scales its inputs to be in the range $x \\in (0, 1)$. As such, samples from $\\eta_{pr}$ are of appropriate scale following transformation. Because values of $\\beta$ are normally much larger than 1, a scaling function, $C$, is used to scale the samples of $\\beta_{pr}$ to an appropriate range. For example, a user would set $C=20$ to sample $\\beta \\in (0,20)$.\n",
    "\n",
    "For details, see the [Stan users manual](http://mc-stan.org/users/documentation/) (section 28.6). The [*hBayesDM*](https://github.com/CCS-Lab/hBayesDM) and [*fitr*](https://github.com/abrahamnunes/fitr) packages provided the templates for fitting these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from scripts.utilities import phi_approx\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook', font_scale=1.75)\n",
    "%matplotlib inline\n",
    "\n",
    "## Initialize canvas.\n",
    "fig, axes = plt.subplots(1,2,figsize=(12,3))\n",
    "x = np.linspace(-5,5,101)\n",
    "\n",
    "## Plot Phi_approx function.\n",
    "axes[0].plot(x, phi_approx(x), lw=2.5)\n",
    "axes[0].set(xlabel='x', ylabel='Phi_approx(x)', title='Transform')\n",
    "\n",
    "## Plot prior.\n",
    "axes[1].plot(phi_approx(x), norm(0,1).pdf(x))\n",
    "axes[1].set(xlabel='Phi_approx(x)', ylabel=r'$N_{(0,1)}(x)$', title='Prior')\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 1:** Visualizing the *Phi_approx* function. (Left) The transformation of arbitrary values, $x$, through *Phi_approx*. (Right) The probability density of *Phi_approx* scaled values, $x$, sampled from a unit normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical parameterization\n",
    "Non-centered parameterizations are especially useful for hierarchical models, where parameters may vary dramatically in scale (resulting in sampling issues). As such, all subject parameters are similarly sampled from a unit normal prior and transformed. To make the model hierarchical, i.e. to enforce partial pooling, subject-level parameters ($\\beta_i, \\eta_i$) are estimated using a similar two-step procedure:\n",
    "\n",
    "> $\\beta_{pr, i} \\ \\text{~} \\mathcal{N}(0,1) $ \n",
    "\n",
    "> $\\beta_i = C \\cdot \\text{Phi_approx}(\\beta_{pr,\\mu} + \\sigma_{\\beta} \\cdot \\beta_{pr,i})$\n",
    "\n",
    "> $\\eta_{pr,i} \\ \\text{~} \\mathcal{N}(0,1) $ \n",
    "\n",
    "> $\\eta_i = \\text{Phi_approx}(\\eta_{pr,\\mu} + \\sigma_{\\eta} \\cdot \\eta_{pr,i}) $\n",
    "\n",
    "where $\\beta_{pr,\\mu}$ and $\\eta_{pr,\\mu}$ are the group-level inverse temperature and learning rates, respectively; and $\\sigma$ captures the variance of the subject-level parameters around the group mean. As above, the group-level parameters are also sampled through a unit normal prior. The variances can be sampled through a half-cauchy distribution or gamma distribution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randomly generate arbitrary subject-level parameters.\n",
    "np.random.seed(47404)\n",
    "theta = np.random.normal(0,1,10)\n",
    "sigma = [0.5,1,1.5]\n",
    "\n",
    "## Plot.\n",
    "fig, ax = plt.subplots(1,1,figsize=(12,3))\n",
    "for s in sigma:\n",
    "    ax.scatter(phi_approx(0 + s * theta), np.ones_like(theta)*s)\n",
    "ax.scatter(phi_approx(np.zeros(3)), [0.5,1,1.5], s=130, marker='d', color='k')\n",
    "    \n",
    "## Add info.\n",
    "ax.hlines(sigma, 0, 1, lw=0.5, zorder=0)\n",
    "ax.set(xlim=(0,1), xticks=np.linspace(0,1,5), xlabel=r'Parameter ($\\theta$)', \n",
    "       ylim=(0.2, 1.8), yticks=sigma, ylabel=r'Variance ($\\sigma$)')\n",
    "    \n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 2:** Visualizing the hierarchical model. Arbitrarily sampled subject-level variables, $\\theta_i$ (color circles), are distributed around a group-level variable, $\\theta_\\mu$ (diamonds). The variance of their distribution is dictated by $\\sigma$. When $\\sigma < 1$, the subject-level parameters shrink towards group-level value; when $\\sigma > 1$, the subject-level parameters move away from group-level value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior on the mood-bias parameter ($f$) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(47404)\n",
    "\n",
    "## Simulate data.\n",
    "f_pr = np.random.multivariate_normal(np.zeros(2), np.identity(2), int(1e6))\n",
    "divisor = [1.5, 1.75, 2.0]\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(12,4))\n",
    "for d in divisor:\n",
    "    \n",
    "    ## Compute f (assumings sigma = 1).\n",
    "    f = np.exp( np.sum(f_pr, axis=1) / d )\n",
    "    \n",
    "    ## Plot.\n",
    "    sns.kdeplot(f, color='w', ax=ax)\n",
    "    line = ax.lines[-1]\n",
    "    xdata, ydata = line.get_xdata(), line.get_ydata()\n",
    "    ax.plot(xdata, ydata/ydata.max(), lw=2.5, label=d)\n",
    "    \n",
    "ax.set_xscale('log')\n",
    "ax.set(xlim=(0.05, 50), xticks=np.logspace(-1,2,4), xlabel=r'$e^{(x_\\mu + x_i)/d)}$', \n",
    "       ylim=(0,1.05), xticklabels=['0.10','1.00','10.0','100.'])\n",
    "ax.legend(loc=2)\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, pystan\n",
    "import _pickle as cPickle\n",
    "from pandas import DataFrame, read_csv\n",
    "from scripts.utilities import check_div, check_energy, check_treedepth, initialize_params\n",
    "%load_ext jupyternotify\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Select model.\n",
    "model_name = False\n",
    "\n",
    "## Sampling parameters.\n",
    "samples = 1250\n",
    "warmup = 1000\n",
    "chains = 4\n",
    "thin = 1\n",
    "n_jobs = 4\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Load and prepare data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Load and prepare behavior data.\n",
    "data = read_csv('data/moodRL_data.csv')\n",
    "data = data[data.Block < 4]\n",
    "data = data.fillna(-1)\n",
    "\n",
    "## Load and prepare ratings data.\n",
    "ratings = read_csv('data/moodRL_ratings.csv')\n",
    "ratings = ratings[ratings.Variable=='Mood']\n",
    "\n",
    "## Load and prepare metadata.\n",
    "metadata = read_csv('data/moodRL_metadata.csv')\n",
    "\n",
    "## Define metadata.\n",
    "N = data.Datetime.unique().size\n",
    "B = data.Block.max()\n",
    "T = data.Trial.max()\n",
    "dt = data.Datetime.unique()\n",
    "\n",
    "## Extract and prepare task data.\n",
    "X = data[['M1','M2']].values.reshape(N,B,T,2)\n",
    "Y = data.Choice.values.reshape(N,B,T,1)\n",
    "Y = np.where( np.any(np.equal(X, Y), axis=-1), np.argmax(np.equal(X, Y), axis=-1) + 1, -1 )\n",
    "R = data.Outcome.values.reshape(N,B,T)\n",
    "R = np.where(R > 0, 1, 0)\n",
    "\n",
    "## Extract and prepare mood data.\n",
    "M = ratings.loc[ratings.Trial>0, 'Rating'].values.reshape(N,B,3) / 4\n",
    "\n",
    "h12 = ratings.loc[ratings.Trial==0, 'Rating'].values.reshape(N,2) / 4\n",
    "h12 = np.where(h12==-1, -0.99, np.where(h12==1, 0.99, h12))\n",
    "h12 = np.arctanh(h12)\n",
    "\n",
    "## Extract and prepare WoF data.\n",
    "WoF = np.sign(metadata.WoF.values) * 28\n",
    "\n",
    "## Organize data dictionary.\n",
    "dd = dict(N=N, B=B, T=T, X=X, Y=Y, R=R, M=M, h12=h12, WoF=WoF)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Fit model with Stan.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "if model_name:\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Model fitting and diagnostics.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    ## Define control parameters.\n",
    "    if 'mood_bias' in model_name: control = dict(adapt_delta = 0.9)\n",
    "    else: control = None\n",
    "\n",
    "    ## Define initialization parameters.\n",
    "    if 'mood_bias_orig' in model_name: init = initialize_params(chains)\n",
    "    else: init = 'random'\n",
    "        \n",
    "    ## Fit model.\n",
    "    file = 'stan_models/%s' %model_name\n",
    "    fit = pystan.stan(file=file, data=dd, iter=samples, warmup=warmup, thin=thin, chains=chains, \n",
    "                      init=init, control=control, n_jobs=n_jobs, seed=47404)\n",
    "    check_div(fit); check_treedepth(fit); check_energy(fit)\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Save data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Create out-directory.\n",
    "    out_dir = 'stan_fits/%s' %model_name.replace('.stan','')\n",
    "    if not os.path.isdir(out_dir): os.makedirs(out_dir)\n",
    "\n",
    "    ## Save summary file.\n",
    "    summary = fit.summary()\n",
    "    summary = DataFrame(summary['summary'], columns=summary['summary_colnames'], index=summary['summary_rownames'])\n",
    "    summary.to_csv(os.path.join(out_dir, 'summary.csv'))\n",
    "\n",
    "    ## Save contents of StanFit.\n",
    "    extract = fit.extract()\n",
    "    for k, v in dd.items(): extract[k] = v\n",
    "    with open(os.path.join(out_dir, 'StanFit.pickle'), 'wb') as f: cPickle.dump(extract, f)\n",
    "        \n",
    "    print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%notify\n",
    "fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior predictive check\n",
    "Drawing inspiration from [Gelman (2000)](http://www.stat.columbia.edu/~gelman/research/published/dogs.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.psis import psisloo\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Define Stan models.\n",
    "model_names = ['moodRL_npool_base.stan', \n",
    "               'moodRL_ppool_base.stan',\n",
    "               'moodRL_ppool_mood.stan',\n",
    "               'moodRL_ppool_mood_bias_mod.stan',\n",
    "#                'moodRL_ppool_mood_bias_orig.stan'\n",
    "              ]\n",
    "  \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Initialize canvas.\n",
    "fig = plt.figure(figsize=(12,9))\n",
    "sns.set_context('notebook', font_scale=1.5)\n",
    "colors = sns.color_palette(n_colors=len(model_names))\n",
    "\n",
    "## Customize axes with GridSpec.\n",
    "gs = gridspec.GridSpec(3, 1)\n",
    "gs.update(left=0.05, right=0.95, bottom=0.35,  top=0.95, hspace=0.05)\n",
    "ax1, ax2, ax3 = plt.subplot(gs[0]), plt.subplot(gs[1]),  plt.subplot(gs[2])\n",
    "\n",
    "gs = gridspec.GridSpec(1, 3)\n",
    "gs.update(left=0.05, right=0.95, bottom=0.05,  top=0.225, hspace=0)\n",
    "ax4, ax5, ax6 = plt.subplot(gs[0]), plt.subplot(gs[1]), plt.subplot(gs[2])\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    \n",
    "    ## Load StanFit file.\n",
    "    f = 'stan_fits/%s/StanFit.pickle' %model_name.replace('.stan','')\n",
    "    with open(f, 'rb') as f: extract = cPickle.load(f)\n",
    "        \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Plot group-level performance.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    ## Extract data and compute optimal choice.\n",
    "    optimal_choice = np.argmax(extract['X'], axis=-1)\n",
    "    Y_obs = np.equal(extract['Y']-1, optimal_choice).astype(int)\n",
    "    Y_pred = np.array([np.equal(sample, optimal_choice) for sample in extract['Y_pred']-1]).astype(int)\n",
    "    \n",
    "    ## Mask missing data.\n",
    "    missing = extract['Y'] < 0\n",
    "    Y_obs = np.where(missing, np.nan, Y_obs)\n",
    "    Y_pred = np.array([np.where(missing, np.nan, sample) for sample in Y_pred])\n",
    "    \n",
    "    ## Compute average over subjects.\n",
    "    Y_obs = np.nanmean(Y_obs, axis=0)\n",
    "    Y_pred = np.apply_over_axes(np.nanmean, Y_pred, [0,1]).squeeze()\n",
    "    n_blocks, n_trials = Y_obs.shape\n",
    "    \n",
    "    for block in np.arange(n_blocks):\n",
    "        \n",
    "        ## Define trial numbers.\n",
    "        trials = np.arange(n_trials) + block * n_trials\n",
    "        trials += 1\n",
    "        \n",
    "        ## Plot.\n",
    "        if not i: ax1.plot( trials, Y_obs[block], lw=1.5, color='k', alpha=0.8 )\n",
    "        ax1.plot( trials, Y_pred[block], lw=2.5, color=colors[i], alpha=0.8 )\n",
    "        \n",
    "    ## Add info.\n",
    "    ax1.vlines([42.5, 84.5], 0, 1, lw=1.5, color='k', zorder=10)\n",
    "    ax1.set(xlim=(0.5, 126), xticks=[], xlabel='', ylim=(0.35, 1.01), \n",
    "            ylabel='Optimal Choice', title='Posterior Predictive Checks')\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Plot group-level mood change.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    ## Extract mood data.\n",
    "    M_obs = extract['M']\n",
    "    M_pred = np.tanh( np.median(extract['h_pred'], axis=0) )\n",
    "    \n",
    "    ## Compute average across subjects by WoF outcome.\n",
    "    WoF = extract['WoF']\n",
    "    M_obs = np.array([M_obs[np.sign(WoF) == v].mean(axis=0) for v in [1,-1]])\n",
    "    M_pred = np.array([M_pred[np.sign(WoF) == v].mean(axis=0) for v in [1,-1]])\n",
    "        \n",
    "    for block in np.arange(n_blocks):\n",
    "        \n",
    "        ## Define trial numbers.\n",
    "        trials = np.array([7,21,35]) + block * n_trials\n",
    "        trials = np.arange(n_trials) + block * n_trials + 1\n",
    "        \n",
    "        for outcome, ax in enumerate([ax2, ax3]):\n",
    "            \n",
    "            if not i: ax.scatter(trials[[6,20,34]], M_obs[outcome, block], s=150,\n",
    "                                 marker='d', color='k', zorder=10)\n",
    "            ax.plot(trials, M_pred[outcome, block], lw=2.5, color=colors[i], alpha=0.8)\n",
    "    \n",
    "    ## Add info.\n",
    "    for ax, ylim, title in zip([ax2,ax3], [(-0.1, 1.1), (-0.6,0.6)], ['WoF Win', 'WoF Loss']):\n",
    "        ax.vlines([42.5, 84.5], -3, 3, lw=1.5, color='k', zorder=10)\n",
    "        ax.hlines(0, 0, n_trials*n_blocks+1, linestyle='--', alpha=0.1, zorder=0)\n",
    "        ax.set(xlim=(0.5,126), xticks=[], ylim=ylim, ylabel='Mood')\n",
    "        ax.annotate(title, (0,0), (0.5,0.9), xycoords='axes fraction', ha='center', fontsize=16)\n",
    "    ax.set(xticks=np.arange(7,126,14), xlabel='Trial')\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Plot WAIC.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    ## Extract log-likelihood values.\n",
    "    Y_log_lik = extract['Y_log_lik']\n",
    "    M_log_lik = extract['M_log_lik']\n",
    "    n_samp, n_subj, n_block, n_trial = Y_log_lik.shape\n",
    "    \n",
    "    ## Reshape data.\n",
    "    Y_log_lik = Y_log_lik.reshape(n_samp, n_subj*n_block*n_trial)\n",
    "    M_log_lik = M_log_lik.reshape(n_samp, n_subj*n_block*3)\n",
    "    \n",
    "    ## Remove log-likelihoods corresponding to missing data.\n",
    "    Y_log_lik = np.where(Y_log_lik, Y_log_lik, np.nan)\n",
    "    missing = np.isnan(Y_log_lik).mean(axis=0) > 0\n",
    "    Y_log_lik = Y_log_lik[:,~missing] \n",
    "    \n",
    "    ## Compute PSIS-LOO.\n",
    "    Y_loo, _, _ = psisloo(Y_log_lik)\n",
    "    M_loo, _, _ = psisloo(M_log_lik)\n",
    "    T_loo, _, _ = psisloo(np.concatenate([Y_log_lik, M_log_lik], axis=-1))\n",
    "    \n",
    "    ## Plot.\n",
    "    ax4.bar(i, -2*Y_loo, 1, color=colors[i])\n",
    "    ax4.set(ylim=(3650,3750), ylabel='PSIS-LOO', title='Model Fits: Choice')\n",
    "    \n",
    "    ax5.bar(i, -2*M_loo, 1, color=colors[i])\n",
    "    ax5.set(title='Model Fits: Mood')\n",
    "        \n",
    "    ax6.bar(i, -2*T_loo, 1, color=colors[i])\n",
    "    ax6.set(ylim=(3600),title='Model Fits: Overall')\n",
    "        \n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Load and prepare data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    " ## Load mood StanFit file.\n",
    "f = 'stan_fits/moodRL_ppool_mood/StanFit.pickle'\n",
    "with open(f, 'rb') as f: fit1 = cPickle.load(f)\n",
    "    \n",
    "## Load mood bias (mod) StanFit file.\n",
    "f = 'stan_fits/moodRL_ppool_mood_bias_mod/StanFit.pickle'\n",
    "with open(f, 'rb') as f: fit2 = cPickle.load(f)\n",
    "    \n",
    "## Load mood bias (orig) StanFit file.\n",
    "f = 'stan_fits/moodRL_ppool_mood_bias_orig/StanFit.pickle'\n",
    "with open(f, 'rb') as f: fit3 = cPickle.load(f)\n",
    "    \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Plotting.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Initialize canvas.\n",
    "fig, axes = plt.subplots(1,4,figsize=(16,4))\n",
    "\n",
    "## Plot beta.\n",
    "axes[0].scatter(np.median(fit1['beta'], axis=0), np.median(fit2['beta'], axis=0))\n",
    "axes[0].scatter(np.median(fit1['beta'], axis=0), np.median(fit3['beta'], axis=0))\n",
    "axes[0].plot(np.arange(20), np.arange(20), color='k', linestyle='--', zorder=0)\n",
    "axes[0].set(xlim=(0,20), xlabel='No Mood Bias', ylim=(0,20), ylabel='Full Mood', title=r'$\\beta$')\n",
    "\n",
    "## Plot eta_v.\n",
    "axes[1].scatter(np.median(fit1['eta_v'], axis=0), np.median(fit2['eta_v'], axis=0))\n",
    "axes[1].scatter(np.median(fit1['eta_v'], axis=0), np.median(fit3['eta_v'], axis=0))\n",
    "axes[1].plot(np.arange(2), np.arange(2), color='k', linestyle='--', zorder=0)\n",
    "axes[1].set(xlim=(0,0.5), xlabel='No Mood Bias', ylim=(0,0.5), ylabel='Full Mood', title=r'$\\eta_v$')\n",
    "    \n",
    "## Plot eta_h.\n",
    "axes[2].scatter(np.median(fit1['eta_h'], axis=0), np.median(fit2['eta_h'], axis=0))\n",
    "axes[2].scatter(np.median(fit1['eta_h'], axis=0), np.median(fit3['eta_h'], axis=0))\n",
    "axes[2].plot(np.arange(2), np.arange(2), color='k', linestyle='--', zorder=0)\n",
    "axes[2].set(xlim=(0,0.5), xlabel='No Mood Bias', ylim=(0,0.5), ylabel='Full Mood', title=r'$\\eta_h$')\n",
    "   \n",
    "## Plot f.\n",
    "axes[3].scatter(np.log10(np.median(fit2['f'], axis=0)), np.log10(np.median(fit3['f'], axis=0)))\n",
    "axes[3].plot(np.arange(-1,2), np.arange(-1,2), color='k', linestyle='--', zorder=0)\n",
    "# axes[3].set(xlim=(0,0.5), xlabel='No Mood Bias', ylim=(0,0.5), ylabel='Full Mood', title=r'$\\eta_h$')\n",
    "    \n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code scraps (will soon disappear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load StanFit file.\n",
    "model_name = 'moodRL_ppool_mood_mod.stan'\n",
    "f = 'stan_fits/%s/StanFit.pickle' %model_name.replace('.stan','')\n",
    "with open(f, 'rb') as f: extract = cPickle.load(f)\n",
    "f_mod = np.log10( np.median(extract['f'], axis=0) )\n",
    "    \n",
    "model_name = 'moodRL_ppool_mood_orig.stan'\n",
    "f = 'stan_fits/%s/StanFit.pickle' %model_name.replace('.stan','')\n",
    "with open(f, 'rb') as f: extract = cPickle.load(f)\n",
    "f_orig = np.log10( np.median(extract['f'], axis=0) )\n",
    "\n",
    "df = DataFrame({r'$f_{mod}$':f_mod, r'$f_{orig}$':f_orig, \n",
    "                'HPS':metadata.IPIP_hps.values,\n",
    "                r'Mood $\\mu$':M.reshape(31,9).mean(axis=-1),\n",
    "                r'Mood $\\sigma$':M.reshape(31,9).std(axis=-1)})\n",
    "\n",
    "g = sns.PairGrid(df)\n",
    "g.map_diag(plt.hist)\n",
    "g.map_lower(sns.regplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group-level mood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Prepare data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Extract.\n",
    "h_pred = extract['h_pred'][...,[6,20,34]]\n",
    "n_samp, n_subj, n_block, n_trial = h_pred.shape\n",
    "\n",
    "M_pred = np.median(np.tanh(h_pred), axis=0).reshape(n_subj, n_block * 3)\n",
    "M_obs = extract['M'].reshape(n_subj, n_block * n_trial)\n",
    "\n",
    "## Sort.\n",
    "ix = np.argsort(M_obs.mean(axis=-1))\n",
    "M_obs = M_obs[ix]\n",
    "M_pred = M_pred[ix]\n",
    "\n",
    "## Store as table.\n",
    "data = DataFrame(np.vstack([M_obs.flatten(), M_pred.flatten()]).T,\n",
    "                 columns=('Observed','Predicted'))\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Plot group-level mood checks.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Initialize canvas.\n",
    "fig, axes = plt.subplots(3,1,figsize=(12,9))\n",
    "cmap = sns.diverging_palette(250, 10, s=90, l=50, center=\"dark\", as_cmap=True)\n",
    "\n",
    "## Plot mood heatmaps.\n",
    "for ax, arr, title in zip(axes[:2], [M_obs, M_pred], ['Observed','Predicted']):\n",
    "    \n",
    "    ## Setup colorbar.\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    \n",
    "    ## Plot heatmap.\n",
    "    sns.heatmap(arr, vmin=-1, vmax=1, cmap=cmap, \n",
    "                cbar_kws=dict(label='Mood'), ax=ax, cbar_ax=cax)\n",
    "    \n",
    "    ## Add info.\n",
    "    ax.hlines(np.arange(ix.size), 0, 9, lw=0.05, color='w')\n",
    "    ax.vlines([3,6], 0, ix.size, lw=5, color='w')\n",
    "    ax.set(xticks=[1.5,4.5,7.5], xticklabels=['Block 1','Block 2','Block 3'], \n",
    "           ylabel='Participants', title=title)\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    ## Update colorbar.\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.set_ticks(np.linspace(-1,1,5) * 0.85)\n",
    "    cbar.ax.set_yticklabels(['%0.2f' %i for i in np.linspace(-1,1,5)], fontsize=14)\n",
    "    \n",
    "## Plot scatterplot.\n",
    "m = np.unique(data.Observed)\n",
    "cmap = sns.diverging_palette(250, 10, s=90, l=50, n=m.size, center=\"dark\")\n",
    "\n",
    "sns.stripplot('Observed', 'Predicted', data=data, palette=cmap, jitter=True, alpha=0.3, ax=axes[2])\n",
    "for x, y, color in zip(range(ix.size), m, cmap): \n",
    "    axes[2].scatter(x,y,s=300,marker='d',color=color,edgecolor='w',lw=1.5)\n",
    "    \n",
    "## Add info.\n",
    "axes[2].set(ylim=(-1.05,1.05), yticks=np.linspace(-1,1,5))\n",
    "axes[2].annotate(r'$r = %0.3f$' %pearsonr(M_obs.flatten(), M_pred.flatten())[0], xy=(0,0),\n",
    "                xytext=(0.025,0.90), xycoords='axes fraction', fontsize=18)\n",
    "sns.despine(ax=axes[2])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group-level parameter pair plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assemble group-level parameters into table.\n",
    "data = DataFrame(np.vstack([extract['mu_beta'],extract['mu_eta_v'],\n",
    "                            extract['mu_eta_h'],extract['mu_f']]).T,\n",
    "                 columns=(r'$\\beta$', r'$\\eta_v$',r'$\\eta_h$',r'$f$'))\n",
    "\n",
    "## Plot.\n",
    "g = sns.PairGrid(data, aspect=1.5)\n",
    "g.map_lower(plt.scatter, s=25, linewidth=1.5, color='none', edgecolor='#1f77b4', alpha=0.4)\n",
    "g.map_upper(sns.kdeplot, cmap=\"Blues_d\")\n",
    "g.map_diag(sns.distplot, kde=False, hist_kws=dict(edgecolor='w', linewidth=0.5, alpha=0.9));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assemble group-level parameters into table.\n",
    "data = DataFrame(extract['sigma'], columns=(r'$\\sigma_\\beta$', r'$\\sigma_v$', \n",
    "                                            r'$\\sigma_h$',r'$\\sigma_f$'))\n",
    "\n",
    "## Plot.\n",
    "g = sns.PairGrid(data, aspect=1.5)\n",
    "g.map_lower(plt.scatter, s=25, linewidth=1.5, color='none', edgecolor='#1f77b4', alpha=0.4)\n",
    "g.map_upper(sns.kdeplot, cmap=\"Blues_d\")\n",
    "g.map_diag(sns.distplot, kde=False, hist_kws=dict(edgecolor='w', linewidth=0.5, alpha=0.9));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Prepare data.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Load and extract HPS data.\n",
    "metadata = read_csv('data/moodRL_metadata.csv')\n",
    "HPS = metadata.IPIP_hps\n",
    "\n",
    "## Extract data.\n",
    "f = np.median(extract['f'], axis=0)\n",
    "M_pred = extract['M'].reshape(f.size, 9)\n",
    "\n",
    "## Store data.\n",
    "xdata = [np.median(M_pred, axis=-1), np.std(M_pred, axis=-1), HPS]\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Plot mood bias parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Initialize canvas.\n",
    "fig, axes = plt.subplots(1,3,figsize=(12,4),sharey=True)\n",
    "xlabels = ['Mood Average', 'Mood Variance', 'HPS']\n",
    "\n",
    "for i, ax, arr, xlabel in zip(range(3), axes, xdata, xlabels):\n",
    "    \n",
    "    ## Plot.\n",
    "    sns.regplot(arr, np.log10(f), ax=ax)\n",
    "\n",
    "    ## Add info.\n",
    "    ax.set(xlabel=xlabel, yticks=[-1,0,1], yticklabels=[0.1, 1.0, 10.], ylim=(-1.2, 1.5));\n",
    "    ax.annotate(r'$r_s = %0.3f, p = %0.3f$' %spearmanr(arr, np.log10(f)), xy=(0,0),\n",
    "                xytext=(1,0.05), xycoords='axes fraction', fontsize=16, ha='right')\n",
    "    if not i: ax.set_ylabel(r'Mood bias ($f$)')\n",
    "\n",
    "ax.set_xlim(1.3, 4)\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {
    "height": "137px",
    "width": "170px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
